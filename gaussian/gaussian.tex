\subsubsection*{Векторные случайные величины}

Векторная случайная величина - это просто вектор-столбец из случайных величин:

\begin{equation}
\vec{X}=\left(\begin{array}{c} X_{1} \\ X_{2} \\ ... \\ X_{n} \\  \end{array} \right)
\end{equation}

В принципе можно говорить о векторе-строке или о случайной матрице, но нам понадобятся только вектор-столбцы. Две самых распространенных характеристики одномерной случайной величины $X$ - это среднее $\E(X)$ и дисперсия $\Var(X)$.
У векторных случайных величин также есть среднее:

\begin{equation}
\E(\vec{X})=\left(\begin{array}{c} \E(X_{1}) \\ \E(X_{2}) \\ ... \\ \E(X_{n}) \\  \end{array} \right)
\end{equation}



И ковариационная матрица:

\begin{equation}
\Var(\vec{X})=\left(
\begin{array}{cccc} 
\Var(X_{1}) & \Cov(X_{1},X_{2}) & ... & \Cov(X_{1},X_{n}) \\ 
\Cov(X_{2},X_{1}) & \Var(X_{2}) & ... & \Cov(X_{2},X_{n}) \\ 
... &&&\\ 
\Cov(X_{n},X_{1}) & \Cov(X_{n},X_{1})  & ... & \Var(X_{n})\\ 
\end{array} 
\right)
\end{equation}

В ковариационной матрицы в $i$-ой строке в $j$-ом столбце находится $\Cov(X_{i},X_{j})$. На диагонали $i$-ый элемент, это $\Cov(X_{i},X_{i})=\Var(X_{i})$. Из-за этого сразу следует, что ковариационная матрица симметрична.

Для простоты мы будем обозначать транспонирование с помощью штриха, вот так: $A'$. Путаницы с производной не возникнет.

С помощью транспонирования легко сказать, что ковариационная матрица симметрична: $\Var(X)'=\Var(X)$.

Конечно же, дисперсию можно записать через математическое ожидание: $\Var(X)=E[(X-\E(X))^{2}]=\E(X^{2})-\E(X)^{2}$

Для ковариационной матрицы это выглядит так:
\begin{equation}
\Var(\vec{X})=E[(X-\E(X))\cdot (X-\E(X))']=\E(XX')-\E(X)\cdot \E(X)'
\end{equation}

Упр. Убедитесь в этом.


Мы быстренько пройдемся по свойствам этих вещей:

Если $A$ - неслучайная матрица и $b$ - неслучайный вектор подходящих размеров, то:
\begin{enumerate}
\item $\E(A\vec{X}+b)=A\E(\vec{X})+b$
\item $\E(\vec{X}A+b)=\E(\vec{X})A+b$
\item $\Var(AX+b)=A\Var(X)A'$.
\item $\Var(X)$ - неотрицательно определена, имеет $n$ неотрицательных собственных чисел (с учетом кратности), где $n$ размерность вектора $X$.
\end{enumerate}

Проверка этих свойств - еще одно упражнение по линейной алгебре.


\subsubsection*{Краткая схема разных определений}

Есть три с половиной подхода определить многомерное нормальное распределение.

Вектор $\vec{X}$ имеет многомерное нормальное распределение, если выполнено одно из трех равносильных условий:
\begin{enumerate}
\item Вектор $\vec{X}$ представим в виде $\vec{X}=A\vec{Z}+\mu$, где $\vec{Z}$ --- это вектор независимых стандартных нормальных случайных величин
\item Любая линейная комбинация $c_{1}X_{1}+c_{2}X_{2}+...+c_{n}X_{n}$ имеет одномерное нормальное распределение
\item Характеристическая функция вектора $\vec{X}$ имеет вид:
\begin{equation}
\phi(\vec{u})=\exp\left(i\cdot \mu'\cdot u-\frac{1}{2}u'Vu \right)
\end{equation}
\end{enumerate}

В случае $det(V)\neq 0$ первые три формулировки становятся эквивалентны четвертой:

Вектор $\vec{X}$ имеет невырожденное многомерное нормальное распределение, если
\begin{enumerate}
\item Функция плотности вектора $\vec{X}$ имеет вид:
\begin{equation}
p(\vec{x})=(2\pi)^{-n/2}(\det(V))^{-1/2}\cdot \exp\left(-\frac{1}{2}(\vec{x}-\mu) V^{-1} (\vec{x}-\mu)' \right) 
\end{equation}
\end{enumerate}

Мы выбираем первое свойство как определение, а остальные будет доказывать как теоремы.


\subsubsection*{Определение и функция плотности}


Очень часто бывает удобно говорить о константе как о нормально распределенной случайной величине с нулевой дисперсией. Это просто соглашение. Никакой функции плотности у константы конечно же нет! Просто когда мы говорим $N(\mu,\sigma^{2})$, $\sigma>0$, мы имеем ввиду <<честное>> нормальное распределение c функцией плотности
\begin{equation}
p(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)
\end{equation}
, а когда говорим $N(\mu,0)$ имеем ввиду просто константу $\mu$.

Т.е. говоря <<нормальное распределение>> мы теперь можем иметь ввиду и константу. Если же нам надо подчеркнуть, что речь идет о <<честном>> нормальном распределении с функцией плотности, то мы будем говорить <<невырожденное нормальное распределение>>.


Теперь мы можем дать несколько необычное определение одномерной нормальной величины... Пусть $a$ и $b$ - это константы.

\begin{mydef}
Если случайная величина $X$ представима в виде $X=aZ+b$, где $Z\sim N(0,1)$, то мы говорим, что $X$ имеет \indef{нормальное распределение} $N(b,a^{2})$. Если, кроме того, $a\neq 0$, то мы говорим, что $X$ имеет \indef{невырожденное нормальное распределение}.
\end{mydef}


Теперь мы готовы сказать, что такое многомерное нормальное распределение. Пусть матрица $A$ - неслучайная, вектор $b$ - неслучайный. Размерности подходящие.

\begin{mydef} \label{def:mult_norm}
Если $X=AZ+b$, где вектор $Z$ состоит из независимых $Z_{i}\sim N(0;1)$, то мы говорим, что вектор $X$ имеет \indef{нормальное распределение} $N(b,AA')$. Если, кроме того, $A$ обратимая матрица, $det(A)\neq 0$, то мы говорим, что $X$ имеет \indef{невырожденное нормальное распределение}.
\end{mydef}

В этой главе и в книге в целом мы \indef{постараемся} уточнять, но обычно из контекста понятно, включает ли автор в понятие нормального распределения вырожденные случаи. Как правило, да, включает.


\begin{myex}
Вектор $X$ имеет вырожденное нормальное распределение, у него нет двумерной функции плотности:

\begin{equation}
\left(\begin{array}{c} X_{1} \\ X_{2}  \end{array} \right)=
\left(\begin{array}{cc} 0 & 1 \\ 0 & 1  \end{array} \right)\cdot
\left(\begin{array}{c} Z_{1} \\ Z_{2}  \end{array} \right)=
\left(\begin{array}{c} Z_{1} \\ Z_{1}  \end{array} \right)
\end{equation}

\end{myex}



Невырожденный нормальный вектор имеет функцию плотности...
\begin{myth}
Вектор $X$ имеет невырожденное нормальное распределение $N(b,AA')$, если и только если его функция плотности имеет вид 
\begin{equation}
p(\vec{x})=(2\pi)^{-n/2}(\det(AA'))^{-1/2}\cdot \exp\left(-\frac{1}{2}(\vec{x}-b) (AA')^{-1} (\vec{x}-b)' \right) 
\end{equation}
Если не обращать внимания на константу, которая нужна чтобы интеграл под функцией плотности равнялся единице, то:
\begin{equation}
p(\vec{x})\sim \exp\left(-\frac{1}{2}(\vec{x}-b) (AA')^{-1} (\vec{x}-b)' \right) 
\end{equation}
\end{myth}

\begin{proof}


\end{proof}



\subsubsection*{Напоминалка про характеристические функции}


Опытный охотник увидев уши, торчащие из кустов, скажет: <<Ага, это заяц!>> Для этих же целей нужна и характеристическая функция. Она позволяет опознать закон распределения случайной величины. 

Узнав например, что характеристическая функция случайной величины $W$ равна $\phi(u)=\frac{\exp(iu)-1}{iu}$, опытный охотник скажет: <<Ага, это равномерная на $[0;1]$!>>. Увидев характеристическую функцию $\phi(u)=(p\cos(u)+ip\sin(u)+1-p)^{n}$, опытный охотник узнает биномиальное распределение $Bin(n,p)$.

Настала пора для формального определения:

\begin{mydef}
Характеристической функцией случайной величины $W$ называется функция $\phi:\mathbb{R}\to \mathbb{C}$:
\begin{equation}
\phi_{W}(u):=\E(\cos(uW))+i\E(\sin(uW))
\end{equation}
Или более кратко:
\begin{equation}
\phi_{W}(u):=\E(\exp(iuW))
\end{equation}
\end{mydef}

Косинус и синус - ограниченные функции, поэтому для любой случаной величины $W$ существуют $\E(\sin(uW))$ и $\E(\cos(uW))$. А значит и характеристическая функция всегда существует.
Тот факт, что по характеристической функции можно узнать закон распределения строго формулируется в виде теоремы:


\begin{myth}
Пусть $X_{1}$ и $X_{2}$ --- две случайной величины. Функции распределения $F_{1}(t)$ и $F_{2}(t)$ совпадают если и только если совпадают характеристические функции $\phi_{1}(u)$ и $\phi_{2}(u)$
\end{myth}









Для описания многомерных нормальных величин характеристическая функция оказывается более удобна, чем функция плотности. Дело в том, что у вырожденных нормальных векторов функции плотности нет, а характеристическая функция существует всегда.



\subsubsection*{Еще два эквивалентных определения}


\begin{myth}
Вектор $X$ имеет нормальное распределение $N(b,AA')$, возможно вырожденное, если и только если его характеристическая функция имеет вид 
\begin{equation}
\phi(\vec{u})=\exp\left(i\cdot \mu'\cdot u-\frac{1}{2}u'Vu \right)
\end{equation}

\end{myth}



Важная особенность многомерного нормального распределения.

\begin{myth} \label{th:normal_one2mult}
Вектор $X=(X_{1},...,X_{n})$ имеет многомерное нормальное распределение (возможно вырожденное) если и только если любая линейная комбинация $Y=c_{1}X_{1}+c_{2}X_{2}+...+c_{n}X_{n}$ имеет одномерное нормальное распределение (возможно вырожденное).
\end{myth}

\begin{proof}
Если $X$ - многомерное нормальное, то $X=AZ+b$. Следовательно, $Y=c_{1}X_{1}+c_{2}X_{2}+...+c_{n}X_{n}=\vec{c}X=\vec{c}AZ+\vec{c}b$. Значит $Y$ по определению \ref{def:mult_norm} имеет нормальное распределение.

Если любая линейная комбинация нормальна...

\end{proof}






\subsubsection*{Стандартизация}

В одномерном случае, если $\Var(X)\neq 0$, то случайную величину $X$ можно <<стандартизировать>>, т.е. превратить в случайную величину с нулевым средним и единичной дисперсией: 

\begin{equation}
Z:=\frac{X-\E(X)}{\sqrt{\Var(X)}}
\end{equation}

У случайной величины $Z$: $\E(Z)=0$, $\Var(Z)=1$.

В многомерном случае, если матрица $\Var(X)$ обратима, то случайный вектор $\vec{X}$ можно <<стандартизовать>>, т.е. превратить в вектор некоррелированных случайных величин с нулевым средним и единичной дисперсией.

\begin{equation}
\vec{Z}:=(\Var(X))^{-1/2}(\vec{X}-\E(\vec{X}))
\end{equation}

Для этой операции нужно понимать, что такое $A^{\frac{1}{2}}$. Для положительно определенной матрицы эта операция корректна. Если $A$ положительно определена, то у нее есть разложение $A=PDP^{-1}$ причем $D$ - диагональная матрица, где на главной диагонали стоят положительные собственные значения $A$. И, стало быть, $A^{\frac{1}{2}}=PD^{\frac{1}{2}}P^{-1}$.

Если нужно решить какую-то задачу, связанную с многомерным нормальным распределением, то стандартизация может здорово облегчить вычисления. Даже в вырожденном случае имеет смысл перейти к рассмотрению независимых нормальных $N(0;1)$ случайных величин.
\begin{myex}
Пусть рост любой женщины имеет распределение $N(165,25)$. А корреляция между ростом матери и ростом дочери равна $\rho$. Какова вероятность того, что дочь высокая (выше среднего роста), если мама - высокая?

Начинаем решать:

Обозначим $X_{1}$ - рост матери, $X_{2}$ - рост дочери. $\P(X_{2}>165|X_{1}>165)=\frac{\P(X_{1}>165\cap X_{2}>165)}{\P(X_{1}>165)}=2\P(X_{1}>165\cap X_{2}>165)$.

Если решать без стандартизации, <<в лоб>>. Вылезает интеграл, который берется усердным трудом...

Если перейти к стандартным $Z_{1}$ и $Z_{2}$...


%\left(\begin{array}{c} X_{1} \\ X_{2} \end{array} \right)\sim 

\end{myex}


\subsubsection*{Многомерное --- это больше чем несколько одномерных}

Из теоремы \ref{th:normal_one2mult} следует в частности, что:

\begin{myth}
Если вектор $\vec{X}$ имеет многомерное нормальное распределение, то и любая его компонента $X_{i}$ имеет нормальное распределение. Кроме того, если распределение $\vec{X}$ --- невырожденное, то и распределение каждой компоненты $X_{i}$ невырожденное.
\end{myth}

Обратное утверждение неверно. Мы приведем пример, в котором $X_{1}$ и $X_{2}$ имеют нормальное распределение по отдельности, но не имеют совместного нормального распределения.

\begin{myex}
Пусть $X_{1}\sim N(0;1)$, а $K$ --- это случайная величина равновероятно принимающая значения $1$ и $-1$, причем $K$ и $X_{1}$ независимы. Определим
\begin{equation}
X_{2}=K\cdot |X_{1}|
\end{equation}
Проверим, что $X_{2}$ имеет нормальное распределение. Для положительных $x$:
\begin{multline}
\P(X_{2}\leq x)=\P(K=-1)+\P(K=1)\cdot \P(|X_{1}|\leq x)=\\
=0.5+0.5\P(|X_{1}|\leq x)=0.5+\P(X_{1}\in [0;x])=\P(X_{1}\leq x)=F(x)
\end{multline}
Для отрицательных $x$ проверьте сами!

Можно заметить, что $X_{1}+X_{2}$ не является нормально распределенной случайной величиной. Более того, распределение $X_{1}+X_{2}$ не является непрерывным. Действительно,
\begin{equation}
\P(X_{1}+X_{2}=0)=\P(X_{1}+K|X_{1}|=0)=\P(K\neq \sgn(X_{1}))=0.5
\end{equation}
Если бы вектор $(X_{1}, X_{2})$ имел совместное нормальное распределение, то сумма $X_{1}+X_{2}$ была бы нормально распределенной.
Кстати, найдем ковариацию между $X_{1}$ и $X_{2}$:
\begin{multline}
\Cov(X_{1},X_{2})=\E(X_{1}X_{2})-\E(X_{1})\E(X_{2})=\E(X_{1}X_{2})=\E(X_{1}\cdot K\cdot|X_{1}|)=\\
=\E(K)\E(X_{1}\cdot |X_{1}|)=0
\end{multline}
Значит наши $X_{1}$ и $X_{2}$ были некоррелированны, нормальны по отдельности. Вместе с тем они не были нормальны в совокупности. И конечно, они зависимы, т.к. $|X_{1}|=|X_{2}|$.

\end{myex}


\subsubsection*{Некоррелированность и независимость}
% включена в абзац про связь одномерного и многомерного

Для многомерного нормального распределения некоррелированность равносильна независимости.

Доказательство для двумерного...





\subsubsection*{Условное распределение}

Предположим, что вектор $(X,Y)$ имеет совместное нормальное распределение. Часто возникает задача прогнозирования случайной величины $Y$, если значение случайной величины $X$ известно.

%Для начала мы отметим, что при известном $X$ условное распределение $Y$ также будет нормальным.

Для удобства вычислений всегда используем стандартизацию! Вместо исходных $X$ и $Y$ рассмотрим:
\begin{equation}
Z_{1}=\frac{X-\E(X)}{\sigma_{X}}, \quad Z_{2}=\frac{Y-\E(Y)}{\sigma_{Y}}
\end{equation}
Естественно, $Z_{1}\sim N(0;1)$ и $Z_{2}\sim N(0;1)$ и $\Corr(Z_{1},Z_{2})=\Corr(X,Y)$.

Найдем условную функцию плотности $Z_{2}$ при известном $Z_{1}$:
\begin{equation}
p(z_{2}|z_{1})=\frac{p(z_{1},z_{2})}{p(z_{1})}
\end{equation}

Поскольку нас интересует только зависимость от $z_{2}$, то на $p(z_{1})$ можно не обращать внимания. Значком $\sim$ мы будем обозначать равенство с точностью до константы не зависящей от $z_{2}$:
\begin{multline}
p(z_{2}|z_{1})\sim p(z_{1},z_{2})\sim \exp\left(-\frac{1}{2}(z_{1},z_{2})\left(\begin{matrix}
1 & \rho \\ 
\rho & 1
\end{matrix}\right)^{-1}\left( \begin{matrix}
z_{1} \\ 
z_{2}
\end{matrix}\right)   \right)=\\
=\exp\left(-\frac{1}{2(1-\rho^{2})}(z_{1},z_{2})\left(\begin{matrix}
1 & -\rho \\ 
-\rho & 1
\end{matrix}\right)\left( \begin{matrix}
z_{1} \\ 
z_{2}
\end{matrix}\right)   \right)= \\
=\exp\left(-\frac{1}{2(1-\rho^{2})}\left(z_{2}^{2}-2\rho z_{1}z_{2}+z_{1}^{2}\right) \right)\sim 
\exp\left(-\frac{1}{2(1-\rho^{2})}\left(z_{2}-\rho z_{1}\right)^{2}\right)
\end{multline}

Сравним полученный результат с функцией плотности одномерного нормального распределения:
\begin{equation}
p(x)\sim\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)
\end{equation}

Замечаем, что $\sigma^{2}=1-\rho^{2}$ и $\mu=\rho z_{2}$. И оформляем результат вычислений в виде теоремы:
\begin{myth}
Если $Z_{1}$ и $Z_{2}$ имеют совместное нормальное распределение, $\E(Z_{i})=0$, $\Var(Z_{i})=1$ и $\Corr(Z_{1},Z_{2})=\rho$, то условное распределение $Z_{1}$ при известном $Z_{2}$ является нормальным и:
\begin{equation}
\E(Z_{2}|Z_{1}=z)=\rho\cdot z
\end{equation}
\begin{equation}
\Var(Z_{2}|Z_{1}=z)=1-\rho^{2}
\end{equation}
\end{myth}

Формулы дают нам смысл:
\begin{enumerate}
\item Корреляция стандартизированных нормальных величин показывает на сколько в среднем растет одна случайная величина при росте другой на единицу.
\item Условная дисперсия $Z_{2}$ не зависит от конкретного значения $Z_{1}$. Это несколько неожиданно, но интуитивного объяснения я не знаю.
\end{enumerate}
 

Для возврата к исходным $X$ и $Y$ замечаем, что условие $X=x$ равносильно тому, что $Z_{1}=\frac{x-\mu_{x}}{\sigma_{x}}$ и, кроме того, $Y=\mu_{y}+\sigma_{y}Z_{2}$.

Сделав обратную замену, получаем:
\begin{myth}
Если $X$ и $Y$ имеют совместное нормальное распределение, $X\sim N(\mu_{x},\sigma^{2}_{x})$, $Y\sim N(\mu_{y},\sigma^{2}_{y})$ и $\Corr(X,Y)=\rho$, то условное распределение $Y$ при известном $X$ является нормальным и:
\begin{equation}
\E(Y|X=x)=\mu_{y}+\rho\sigma_{y}\frac{x-\mu_{x}}{\sigma_{x}}
\end{equation}
\begin{equation}
\Var(Y|X=x)=(1-\rho^{2})\sigma_{y}^{2}
\end{equation}
\end{myth}

Получаем трактовку корреляции: Если $X$ и $Y$ имеют совместное нормальное распределение, то корреляция  показывает на сколько своих стандартных отклонений в среднем растет $Y$  при росте $X$ на одно свое стандартное отклонение. 


\subsubsection*{Геометрический смысл ковариационной матрицы}


\begin{mydef}
Матрица $R$ называется \indef{матрицей поворота} если одновременно выполнены два условия:
$R'R=I$ и $\det(R)=1$
\end{mydef}

Почему определение поворота именно такое?

Условие $RR'=I$ означает две вещи:
\begin{enumerate}
\item Вектор $R\vec{x}$ имеет такую же длину, как и вектор $\vec{x}$:

\begin{equation}
|\vec{x}|^{2}=\vec{x}'\vec{x}=\vec{x}'R'R\vec{x}=(R\vec{x})'(R\vec{x})=|R\vec{x}|^{2}
\end{equation}

\item Угол между $\vec{x}$ и $\vec{y}$ равен углу между $R\vec{x}$ и $R\vec{y}$
\begin{equation}
\cos(\vec{x},\vec{y})=\frac{\vec{x}'\vec{y}}{|\vec{x}||\vec{y}|}=
\frac{\vec{x}'R'R\vec{y}}{|R\vec{x}||R\vec{y}|}=
\frac{(R\vec{x})'(R\vec{y})}{|R\vec{x}||R\vec{y}|}=\cos(R\vec{x},R\vec{y})
\end{equation}
\end{enumerate}

Условию $R'R=I$ подходят матрицы с определителем $\det(A)=\pm 1$. Дополнительное  условие $\det(R)=1$ исключает <<отражения>>.


Упражнение. Мы доказали, что матрицы вида $R'R=I$ сохраняют углы и длины. Докажите, что никакие другие матрицы не сохраняют одновременно углы и длины.

Решение. Рассмотрим вектор $e_{k}=(0,0,\ldots,0,1,0,\ldots,0)'$. 

Из линейной алгебры:
\begin{myth}
Если $A_{n\times n}$ действительная симметричная положительно полу-определенная матрица, то
\begin{enumerate}
\item У $A$ имеется ровно $n$ действительных собственных чисел
\item $A$ представима в виде 
\begin{equation}
A=RDR^{-1}=RDR'
\end{equation}, 
где $D$ --- диагональная матрица из собственных чисел матрицы $A$, а $R$ --- матрица поворота из собственных векторов матрицы $A$.
\end{enumerate}
\end{myth}


Для начала сформулируем геометрические факты:
\begin{mydef}
Множество точек называется \indef{эллипсоидом}, если оно задается системой уравнений
\begin{equation}
(\vec{x}-\vec{x}_{0})' A (\vec{x}-\vec{x}_{0})=1
\end{equation}
, где $A$ --- положительно определенная матрица. Точка $\vec{x}_{0}$ --- центр эллипсоида. В двумерном случае эллипсоид называют \indef{эллипсом}.
\end{mydef}


Почему определение эллипса именно такое?

Матрицу $A$ можно представить в виде $A=R'DR$, где $R$ --- матрица поворота, а $D$ --- диагональная матрица собственных чисел матрицы $A$. Отсюда получаем, что уравнение эллипса можно записать в виде:
\begin{equation}
(R(\vec{x}-\vec{x}_{0}))' D (R(\vec{x}-\vec{x}_{0}))=1
\end{equation}
Если ввести обозначения $\vec{z}=R(\vec{x}-\vec{x}_{0})$, то уравнение примет вид:
\begin{equation}
\sum_{i=1}^{n} d_{ii}z_{i}^{2}=1
\end{equation}

Именно в силу этого представления:
\begin{mydef}
Для эллипса $(\vec{x}-\vec{x}_{0})' A (\vec{x}-\vec{x}_{0})=1$ собственные векторы матрицы $A$ называют \indef{направлениями полуосей}. Если $\lambda_{i}$ --- это собственное число матрицы $A$, то величины $1/\sqrt{\lambda_{i}}$ называют \indef{длинами полуосей}.
\end{mydef}


Теперь мы готовы нарисовать линии уровня многомерного нормального распределения!
\begin{myth}
Для невырожденного нормального распределения линии уровня функции плотности являются эллипсоидами. Направления главных осей задаются собственными векторами ковариационной матрицы. Соотношение длин полуосей равно соотношению корней из собственных чисел ковариационной матрицы.
\end{myth}


\begin{proof}

Для доказательства нам потребуется технический факт из линейной алгебры:
\begin{myth}
Eсли $\vec{a}$ собственный вектор для матрицы $V$ с собственным числом $\lambda$, то $\vec{a}$ собственный вектор для матрицы $V^{-1}$ с собственным числом $1/\lambda$.
\end{myth}
\begin{proof}
Если $\vec{a}$ собственный вектор матрицы $V$, то с одной стороны:
\begin{equation}
V^{-1}\cdot (V\cdot \vec{a})=V^{-1} (\lambda \vec{a})=\lambda V^{-1}\vec{a}
\end{equation}

С другой стороны:
\begin{equation}
(V^{-1}\cdot V)\cdot \vec{a}=\vec{a}
\end{equation}

Т.е. $\lambda V^{-1}\vec{a}=\vec{a}$. Или:
\begin{equation}
V^{-1}\vec{a}=\frac{1}{\lambda}\vec{a}
\end{equation}

\end{proof}


Итак, пусть $X\sim N(\vec{\mu};V)$.

Тогда условие
\begin{equation}
p(\vec{x})=const
\end{equation}
после преобразований равносильно тому, что:
\begin{equation}
(\vec{x}-\vec{\mu})\cdot V^{-1}\cdot (\vec{x}-\vec{\mu})'=const
\end{equation}

Т.е. мы получили наше определение эллипсоида с $A=\frac{1}{const}V^{-1}$.

Направления полуосей задаются собственными векторами $A$, значит они совпадают с собственными векторами $V$. 

Длины полуосей обратно пропорциональны корням из собственных чисел $A$, значит они прямо пропорциональны корням из собственных чисел $V$.
\end{proof}


\begin{myex}
Пусть $X\sim N(0;V)$ и 
\begin{equation}
V=
\left(
\begin{array}{cc}
5 & 1 \\ 
1 & 9
\end{array} 
\right)
\end{equation}

Нарисуйте линии уровня функции плотности $p(x_{1},x_{2})$


Решение.

Тут обязательно картинки. Эллипс. Главные оси. С кодом R и Sage!

\end{myex}


В случае независимых нормальных случайных величин с одинаковой дисперсией линиями уровня будут окружности (сферы при более высоких размерностях). Поскольку поворот никак не влияет на линию уровня мы бесплатно получаем следующую теорему:

\begin{myth} \label{th:rotate_normal}
Если матрица $R$ --- это матрица поворота и вектор $\vec{Z}\sim N(\vec{0},I)$ то вектор $R\vec{Z}\sim N(\vec{0},I)$
\end{myth}



\subsubsection*{Хи-квадрат распределение}

Как известно,

\begin{mydef} Если случайная величина $W$ представима в виде $W=\sum_{i=1}^{k}Z_{i}^{2}$, где $Z_{i}$ --- независимые стандартные нормальные случайные величины, то говорят, что $W$ имеет \indef{хи-квадрат распределение c $k$ степенями свободы}.
\end{mydef}


К сожалению, проверять, что что-то имеет хи-квадрат распределение напрямую очень неудобно.

\begin{myex}
Давайте попробуем по определению показать, что если $X_{i}\sim N(\mu;\sigma^{2})$ и независимы, то 
\begin{equation}
\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}\sim \chi_{n-1}^{2}
\end{equation}

Как всегда, сначала стандартизируем наши $X_{i}$. 



...


\end{myex}


Гораздо более удобным оказывается следующий способ:
\begin{myth}
Если вектор $\vec{Z}$ состоит из независимых стандартных нормальных случайных величин и все собственные числа симметричной матрицы $A$ равны либо нулю, либо единице, то 
\begin{equation}
\vec{Z}'A\vec{Z}\sim \chi_{r}^{2}
\end{equation}
где $r$ --- количество собственных чисел матрицы $A$ равных единице.
\end{myth}

\begin{proof}
Матрица $A$ представима в виде $A=R'DR$. Поэтому:
\begin{equation}
\vec{Z}'A\vec{Z}=\vec{Z}'R'DR\vec{Z}=(R\vec{Z})'D(R\vec{Z})
\end{equation}

Остается заметить, что $R\vec{Z}$ --- это вектор независимых стандартных нормальных случайных величин в силу теоремы \ref{th:rotate_normal}. Если обозначить этот вектор буквой $\vec{W}=R\vec{Z}$, то
\begin{equation}
\vec{Z}'A\vec{Z}=\vec{W}'D\vec{W}=\sum_{i=1}^{n}d_{ii}W_{i}^{2}
\end{equation}

Матрица $D$ --- это матрица собственных чисел матрицы $A$, т.е. $d_{ii}$ равны либо нулю, либо единице. Получается, что $\vec{Z}'A\vec{Z}$ --- это сумма $r$ стандартных независимых нормальных случайных величин.
\end{proof}


\begin{myth}
Собственные числа симметричной матрицы $A$ равны либо нулю, либо единице, если и только если  $A^{2}=A$
\end{myth}

\begin{proof}
С одной стороны:
\begin{equation}
A^{2}=(R'DR)(R'DR)=R'D(RR')DR=R'DDR=R'D^{2}R
\end{equation}
С другой стороны $A=R'DR$. Значит $A^{2}=A$ если и только если $D^{2}=D$. Но $D$ --- диагональная матрица, поэтому условие $D^{2}=D$ равносильно тому, что на диагонали стоят либо нолики, либо единички.
\end{proof}


\begin{myex}
Тот же пример только быстрее...
...

\end{myex}


\subsubsection*{Границы на хвостовые вероятности}
При изучении броуновского движения полезны два неравенства.

\begin{myth} Если $Z\sim N(0;1)$, то:
\begin{equation}
\frac{1}{\sqrt{2\pi}}\frac{\exp(-x^{2}/2)}{x+x^{-1}}\leq \P(Z\geq x)\leq \frac{1}{\sqrt{2\pi}}\frac{\exp(-x^{2}/2)}{x}, \quad x>0
\end{equation}
\end{myth}

\begin{proof} Заметим, что $\exp(-t^{2}/2)'=-t\exp(-t^{2}/2)$. Получаем верхнюю границу:
\begin{multline}
\P(Z\geq x)=\int_{x}^{\infty} \frac{1}{\sqrt{2\pi}}\exp(-t^{2}/2)dt\leq \\
\leq \int_{x}^{\infty} \left(\frac{t}{x} \right)\frac{1}{\sqrt{2\pi}}\exp(-t^{2}/2)dt=
\frac{1}{\sqrt{2\pi}}\frac{\exp(-x^{2}/2)}{x}
\end{multline}
Заметим, что $(t^{-1}\exp(-t^{2}/2))'=(1+t^{-2})\exp(-t^{2}/2)$. Получаем нижнюю границу:
\begin{multline}
\P(Z\geq x)=\int_{x}^{\infty} \frac{1}{\sqrt{2\pi}}\exp(-t^{2}/2)dt
\geq \int_{x}^{\infty} \left(\frac{1+t^{-2}}{1+x^{-2}} \right)\frac{1}{\sqrt{2\pi}}\exp(-t^{2}/2)dt= \\
=\frac{1}{\sqrt{2\pi}}\frac{1}{1+x^{-2}}\frac{\exp(-x^{2}/2)}{x}=
\frac{1}{\sqrt{2\pi}}\frac{\exp(-x^{2}/2)}{x+x^{-1}}
\end{multline}

\end{proof}

\begin{myth} Если $Z\sim N(0;1)$, то:
\begin{equation}
\frac{2x}{\sqrt{2\pi e}}\leq \P(|Z|\leq x)\leq \frac{2x}{\sqrt{2\pi}}, \quad 0<x\leq 1
\end{equation}
\end{myth}

\begin{proof}
Если $|t|\leq 1$, то
\begin{equation}
\frac{1}{\sqrt{2\pi e}}\leq \frac{1}{\sqrt{2\pi}}\exp(-t^{2}/2)\leq \frac{1}{\sqrt{2\pi}}
\end{equation}
Интегрируя это неравенство от $-x$ до $x$ получаем требуемое.
\end{proof}


Еще кусок из блога... (ссылка?) ...

\subsubsection*{Откуда взялось $\pi$ в формуле?}

Есть много способов объяснить, откуда берется $\pi$ в формуле функции плотности. Вот то, которое нравится мне.

Рассмотрим пару независимых стандартных нормальных случайных величин $X$ и $Y$. Их функция плотности имеет вид:
\begin{equation}
p(x,y)=c\cdot \exp\left( -\frac{1}{2} (x^{2}+y^{2})\right)
\end{equation}

Рассмотрим более подробно функцию
\begin{equation}
f(x,y)=\exp\left( -\frac{1}{2} (x^{2}+y^{2})\right)
\end{equation}

Значение $f(x,y)$ зависит только от расстояния от точки $(x,y)$ до начала координат. Значит объем под <<шляпой>> является фигурой вращения.

Картинка (слева и справа повернутая):

...

Уже понятно, что $\pi$ --- в деле. Остается вспомнить, что объем фигуры вращения определяется по формуле $Vol=\int_{a}^{b}\pi r^{2}(t)dt$.

В нашем случае: $a=0$, $b=f(0,0)=1$, а $r(t)$ находится из условия:
\begin{equation}
\exp\left( -\frac{1}{2} r^{2}(t)\right)=t
\end{equation}

Находим $r^{2}(t)$ и получаем:
\begin{equation}
r^{2}(t)=-2\ln(t)
\end{equation}

Находим объем фигуры вращения:
\begin{equation}
Vol=\int_{0}^{1} \pi r^{2}(t)dt=\int_{0}^{1} \pi (-2\ln(t))dt=-2\pi\int_{0}^{1}\ln(t)dt=2\pi 
\end{equation}

Интеграл от $\ln(t)$ можно взять либо по частям, либо заметив, что:
\begin{equation}
\int_{0}^{1}\ln(t)dt=-\int_{0}^{\infty}\exp(-t)dt
\end{equation}
Картинка:


Настоящие знатоки берут интеграл $\int_{0}^{\infty}\exp(-t)dt$ по методу Мамикона Мнацаканяна \cite{apostol:visual_calculus} в уме глядя на картинку:



Пояснение к картинке:

Производная функции $\exp(-x)$ равна ей самой умноженной на минус один. Поэтому тень от касательной всегда имеет длину один. Интересующая нас площадь равна площади треугольника плюс оставшаяся площадь. Касательные заметающие оставшуюся площадь можно перенести так, чтобы они замели треугольник. Значит интеграл равен удвоенной площади треугольника.

 
Мы доказали, что объем под функцией $f(x,y)$ равен $2\pi$. Следовательно, функция плотности $p(x,y)$ должна иметь вид:
\begin{equation}
p(x,y)=\frac{1}{2\pi}\cdot \exp\left( -\frac{1}{2} (x^{2}+y^{2})\right)
\end{equation}

Для независимых величин $p(x,y)=p(x)p(y)$, следовательно:
\begin{equation}
p(x)=\frac{1}{\sqrt{2\pi}}\cdot \exp\left( -\frac{1}{2} x^{2}\right)
\end{equation}



\subsubsection*{ЦПТ --- доказательство через характеристические функции}

\begin{myth}
Пусть $X_{n}$ --- последовательность случайных величин с характеристическими функциями $\phi_{n}(u)$. Пусть кроме того, $X$ --- случайная величина с характеристической функцией $\phi(u)$. Последовательность $X_{n}$ сходится по распределению к случайное величине $X$ если и только если последовательность функций $\phi_{n}(u)$ сходится поточечно к функции $\phi(u)$.
\end{myth}


